{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üì∞ Fake News Detection v2 - DistilBERT Training\n",
                "\n",
                "This notebook trains the DistilBERT model on your fake news dataset.\n",
                "\n",
                "**Instructions:**\n",
                "1. Click `Runtime` ‚Üí `Change runtime type` ‚Üí Select `T4 GPU`\n",
                "2. Upload your `Fake.csv` and `True.csv` files when prompted\n",
                "3. Run all cells\n",
                "4. Download the trained model at the end"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 1: Install dependencies\n",
                "!pip install -q transformers datasets accelerate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 2: Upload your CSV files\n",
                "from google.colab import files\n",
                "print(\"üì§ Upload Fake.csv and True.csv files:\")\n",
                "uploaded = files.upload()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 3: Load and prepare data\n",
                "import pandas as pd\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "fake_df = pd.read_csv('Fake.csv')\n",
                "true_df = pd.read_csv('True.csv')\n",
                "\n",
                "fake_df['label'] = 0\n",
                "true_df['label'] = 1\n",
                "\n",
                "df = pd.concat([fake_df, true_df]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
                "\n",
                "print(f\"‚úÖ Total samples: {len(df):,}\")\n",
                "print(f\"   Fake: {len(fake_df):,} | Real: {len(true_df):,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 4: Create Hugging Face Dataset\n",
                "from datasets import Dataset\n",
                "\n",
                "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
                "\n",
                "train_dataset = Dataset.from_pandas(train_df[['text', 'label']])\n",
                "test_dataset = Dataset.from_pandas(test_df[['text', 'label']])\n",
                "\n",
                "print(f\"‚úÖ Train: {len(train_dataset):,} | Test: {len(test_dataset):,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 5: Load tokenizer and tokenize\n",
                "from transformers import DistilBertTokenizer\n",
                "\n",
                "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
                "\n",
                "def tokenize(batch):\n",
                "    return tokenizer(batch['text'], padding='max_length', truncation=True, max_length=512)\n",
                "\n",
                "train_dataset = train_dataset.map(tokenize, batched=True, batch_size=1000)\n",
                "test_dataset = test_dataset.map(tokenize, batched=True, batch_size=1000)\n",
                "\n",
                "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
                "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
                "\n",
                "print(\"‚úÖ Tokenization complete\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 6: Load model\n",
                "from transformers import DistilBertForSequenceClassification\n",
                "\n",
                "model = DistilBertForSequenceClassification.from_pretrained(\n",
                "    'distilbert-base-uncased',\n",
                "    num_labels=2\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Model loaded\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 7: Training setup\n",
                "from transformers import Trainer, TrainingArguments\n",
                "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
                "\n",
                "def compute_metrics(pred):\n",
                "    labels = pred.label_ids\n",
                "    preds = pred.predictions.argmax(-1)\n",
                "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
                "    acc = accuracy_score(labels, preds)\n",
                "    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n",
                "\n",
                "training_args = TrainingArguments(\n",
                "    output_dir='./results',\n",
                "    num_train_epochs=3,\n",
                "    per_device_train_batch_size=16,\n",
                "    per_device_eval_batch_size=32,\n",
                "    warmup_steps=500,\n",
                "    weight_decay=0.01,\n",
                "    logging_steps=100,\n",
                "    eval_strategy='epoch',\n",
                "    save_strategy='epoch',\n",
                "    load_best_model_at_end=True,\n",
                "    metric_for_best_model='f1',\n",
                "    fp16=True,\n",
                "    report_to='none',\n",
                ")\n",
                "\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=train_dataset,\n",
                "    eval_dataset=test_dataset,\n",
                "    compute_metrics=compute_metrics,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 8: TRAIN! (~15-20 minutes with GPU)\n",
                "print(\"üèãÔ∏è Starting training...\")\n",
                "trainer.train()\n",
                "print(\"‚úÖ Training complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 9: Evaluate\n",
                "results = trainer.evaluate()\n",
                "print(\"\\nüìä Final Results:\")\n",
                "print(f\"   Accuracy:  {results['eval_accuracy']:.4f}\")\n",
                "print(f\"   F1 Score:  {results['eval_f1']:.4f}\")\n",
                "print(f\"   Precision: {results['eval_precision']:.4f}\")\n",
                "print(f\"   Recall:    {results['eval_recall']:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 10: Save model\n",
                "model.save_pretrained('./fake_news_distilbert')\n",
                "tokenizer.save_pretrained('./fake_news_distilbert')\n",
                "print(\"‚úÖ Model saved to ./fake_news_distilbert\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 11: Test prediction\n",
                "import torch\n",
                "\n",
                "def predict(text):\n",
                "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
                "    with torch.no_grad():\n",
                "        outputs = model(**inputs.to(model.device))\n",
                "        probs = torch.softmax(outputs.logits, dim=1)[0]\n",
                "    return 'REAL' if probs[1] > probs[0] else 'FAKE', probs.max().item()\n",
                "\n",
                "# Test\n",
                "test_text = \"WASHINGTON (Reuters) - The Senate passed new legislation on healthcare reform.\"\n",
                "pred, conf = predict(test_text)\n",
                "print(f\"\\nüîç Test: {pred} ({conf:.1%} confidence)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 12: Download trained model\n",
                "!zip -r fake_news_distilbert.zip fake_news_distilbert/\n",
                "files.download('fake_news_distilbert.zip')\n",
                "print(\"\\nüì• Download started! Extract to your project folder.\")"
            ]
        }
    ]
}