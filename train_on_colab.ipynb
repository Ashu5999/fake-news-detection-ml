{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üì∞ Fake News Detection v2 - DistilBERT Training\n",
        "## Using Combined Dataset (Original + COVID-19 Data)\n",
        "\n",
        "**Training Time:** ~20 minutes with T4 GPU\n",
        "\n",
        "### Instructions:\n",
        "1. Click `Runtime` ‚Üí `Change runtime type` ‚Üí Select `T4 GPU`\n",
        "2. Upload `combined_training_data.csv` when prompted\n",
        "3. Run all cells\n",
        "4. Download the trained model at the end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Install dependencies\n",
        "!pip install -q transformers datasets accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Upload your combined dataset\n",
        "from google.colab import files\n",
        "print(\"üì§ Please upload 'combined_training_data.csv':\")\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Load and prepare data\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load combined dataset\n",
        "df = pd.read_csv('combined_training_data.csv')\n",
        "\n",
        "# Clean data\n",
        "df = df.dropna(subset=['text'])\n",
        "df = df[df['text'].str.len() > 50]  # Keep substantial texts\n",
        "\n",
        "print(f\"‚úÖ Total samples: {len(df):,}\")\n",
        "print(f\"   Fake (label=0): {(df['label'] == 0).sum():,}\")\n",
        "print(f\"   Real (label=1): {(df['label'] == 1).sum():,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Create Hugging Face Dataset\n",
        "from datasets import Dataset\n",
        "\n",
        "train_df, test_df = train_test_split(\n",
        "    df, test_size=0.2, random_state=42, stratify=df['label']\n",
        ")\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df[['text', 'label']].reset_index(drop=True))\n",
        "test_dataset = Dataset.from_pandas(test_df[['text', 'label']].reset_index(drop=True))\n",
        "\n",
        "print(f\"‚úÖ Train: {len(train_dataset):,} | Test: {len(test_dataset):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: Load tokenizer and tokenize\n",
        "from transformers import DistilBertTokenizer\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(\n",
        "        batch['text'], \n",
        "        padding='max_length', \n",
        "        truncation=True, \n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "print(\"üîÑ Tokenizing... (this may take a few minutes)\")\n",
        "train_dataset = train_dataset.map(tokenize, batched=True, batch_size=1000)\n",
        "test_dataset = test_dataset.map(tokenize, batched=True, batch_size=1000)\n",
        "\n",
        "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "print(\"‚úÖ Tokenization complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 6: Load model\n",
        "from transformers import DistilBertForSequenceClassification\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\n",
        "    'distilbert-base-uncased',\n",
        "    num_labels=2\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Model loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 7: Training setup\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        labels, preds, average='binary'\n",
        "    )\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc, \n",
        "        'f1': f1, \n",
        "        'precision': precision, \n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=100,\n",
        "    eval_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='f1',\n",
        "    fp16=True,  # Mixed precision for faster training\n",
        "    report_to='none',\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Training setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 8: TRAIN! (~15-20 minutes with T4 GPU)\n",
        "print(\"üèãÔ∏è Starting training on COMBINED dataset...\")\n",
        "print(\"   Original (2016-2017) + COVID-19 (2020) data\")\n",
        "print(\"   Expected time: ~15-20 minutes with T4 GPU\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n‚úÖ Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 9: Evaluate\n",
        "results = trainer.evaluate()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"üìä FINAL EVALUATION RESULTS\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"\\n   Accuracy:  {results['eval_accuracy']:.4f}\")\n",
        "print(f\"   F1 Score:  {results['eval_f1']:.4f}\")\n",
        "print(f\"   Precision: {results['eval_precision']:.4f}\")\n",
        "print(f\"   Recall:    {results['eval_recall']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 10: Save model\n",
        "model.save_pretrained('./fake_news_distilbert')\n",
        "tokenizer.save_pretrained('./fake_news_distilbert')\n",
        "print(\"‚úÖ Model saved to ./fake_news_distilbert\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 11: Test prediction\n",
        "import torch\n",
        "\n",
        "def predict(text):\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs.to(model.device))\n",
        "        probs = torch.softmax(outputs.logits, dim=1)[0]\n",
        "    pred = 'REAL' if probs[1] > probs[0] else 'FAKE'\n",
        "    conf = probs.max().item()\n",
        "    return pred, conf\n",
        "\n",
        "# Test with different samples\n",
        "print(\"\\nüîç Testing predictions:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Real news style\n",
        "test1 = \"WASHINGTON (Reuters) - The Senate passed new legislation on healthcare reform.\"\n",
        "pred1, conf1 = predict(test1)\n",
        "print(f\"\\n1. Reuters style: {pred1} ({conf1:.1%})\")\n",
        "\n",
        "# Fake news style\n",
        "test2 = \"SHOCKING! Scientists EXPOSED for LYING about vaccines! The TRUTH they don't want you to know!\"\n",
        "pred2, conf2 = predict(test2)\n",
        "print(f\"2. Sensational: {pred2} ({conf2:.1%})\")\n",
        "\n",
        "# COVID-related\n",
        "test3 = \"Health officials confirmed new COVID-19 cases have decreased following vaccination efforts.\"\n",
        "pred3, conf3 = predict(test3)\n",
        "print(f\"3. COVID factual: {pred3} ({conf3:.1%})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 12: Download trained model\n",
        "!zip -r fake_news_distilbert_combined.zip fake_news_distilbert/\n",
        "files.download('fake_news_distilbert_combined.zip')\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"üì• DOWNLOAD STARTED!\")\n",
        "print(\"=\" * 50)\n",
        "print(\"\\n1. Extract the zip file\")\n",
        "print(\"2. Place 'fake_news_distilbert' folder in your project\")\n",
        "print(\"3. Run: streamlit run app_v2.py\")"
      ]
    }
  ]
}